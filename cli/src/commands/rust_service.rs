//! # Rust Service Release Management
//!
//! This module handles the complete release workflow for Rust microservices:
//! 1. Build with crate2nix (per-crate caching via Attic)
//! 2. Push to GHCR and Attic registries
//! 3. Deploy via GitOps (commit + push to manifest repo)
//! 4. Flux reconciliation
//! 5. Database migrations (PostgreSQL, ClickHouse, or Elasticsearch)
//! 6. GraphQL schema extraction
//! 7. Apollo Federation supergraph composition
//!
//! ## Migration System Requirements
//!
//! Services MUST implement `main.rs` logic to handle the `RUN_MODE` environment variable:
//!
//! - `RUN_MODE=migrate` - Run PostgreSQL migrations using sqlx
//! - `RUN_MODE=migrate_clickhouse` - Run ClickHouse migrations using clickhouse-rs
//! - `RUN_MODE=migrate_elasticsearch` - Run Elasticsearch migrations using elasticsearch-rs
//!
//! Example implementation:
//! ```rust,ignore
//! #[tokio::main]
//! async fn main() -> Result<()> {
//!     match std::env::var("RUN_MODE").as_deref() {
//!         Ok("migrate") => run_postgres_migrations().await,
//!         Ok("MIGRATE") => run_migrations().await, // For Databend (uses sqlx like PostgreSQL)
//!         Ok("migrate_elasticsearch") => run_elasticsearch_migrations().await,
//!         _ => start_server().await,
//!     }
//! }
//! ```

use crate::commands::service_config::{DatabaseType, ServiceConfig};
use crate::config::{resolve_deploy_yaml_path, DeployConfig};
use crate::path_builder::PathBuilder;
use crate::infrastructure::registry::{ArchImage, RegistryClient, RegistryCredentials};
use crate::repo::get_tool_path;
use anyhow::{anyhow, bail, Context, Result};
use colored::Colorize;
use std::env;
use std::path::{Path, PathBuf};
use std::process::Stdio;
use tokio::process::Command;
use tokio::select;

/// Resolve deploy.yaml path from SERVICE_DIR, checking deploy/{service_name}.yaml first.
fn resolve_deploy_yaml_from_service_dir() -> Result<PathBuf> {
    let service_dir = std::env::var("SERVICE_DIR")
        .context("SERVICE_DIR not set - required for deploy.yaml lookup")?;
    let service_dir_path = Path::new(&service_dir);
    let service_name = service_dir_path
        .file_name()
        .and_then(|n| n.to_str())
        .unwrap_or("unknown");

    // Walk up to find product directory
    let product_dir = find_product_dir_from_path(service_dir_path);
    let deploy_path = if let Some(pd) = product_dir {
        resolve_deploy_yaml_path(&pd, service_name, service_dir_path)
    } else {
        service_dir_path.join("deploy.yaml")
    };

    Ok(deploy_path)
}

/// Walk up from a path to find the product directory.
///
/// Monorepo: looks for pkgs/products/{product} pattern.
/// Standalone: looks for a directory with both deploy.yaml and .git (repo root IS product dir).
fn find_product_dir_from_path(start: &Path) -> Option<PathBuf> {
    let mut current = start.to_path_buf();
    loop {
        // Monorepo pattern: pkgs/products/{product}
        if let Some(parent) = current.parent() {
            if let Some(grandparent) = parent.parent() {
                if parent.file_name().and_then(|n| n.to_str()) == Some("products")
                    && grandparent.file_name().and_then(|n| n.to_str()) == Some("pkgs")
                {
                    return Some(current);
                }
            }
        }
        // Standalone repo pattern: directory with deploy.yaml + .git
        if current.join("deploy.yaml").exists() && current.join(".git").exists() {
            return Some(current);
        }
        if let Some(parent) = current.parent() {
            current = parent.to_path_buf();
        } else {
            return None;
        }
    }
}

/// Check if Cargo.nix exists (should be generated by Nix flake before calling this)
fn check_cargo_nix_exists() -> Result<()> {
    if !Path::new("Cargo.nix").exists() {
        bail!(
            "‚ùå Cargo.nix not found!\n\
             \n\
             This should have been generated automatically by the Nix flake.\n\
             If you see this error, the flake wrapper needs to be updated.\n\
             \n\
             Manual workaround:\n\
                nix run .#generateCargoNix\n\
                git add Cargo.nix\n\
                git commit -m 'chore: add Cargo.nix'"
        );
    }
    Ok(())
}

/// Check if an environment variable is set and non-empty
fn check_token(var_name: &str) -> bool {
    env::var(var_name)
        .map(|val| !val.is_empty())
        .unwrap_or(false)
}

/// Get git hash for tagging - Single source of truth
///
/// CRITICAL: To avoid the "one-cycle lag" bug where deploy commits shift HEAD,
/// this function checks for RELEASE_GIT_SHA environment variable FIRST.
/// The Nix release wrapper captures this at the START of the release, before
/// any git commits are made.
///
/// Priority:
/// 1. RELEASE_GIT_SHA env var (set by Nix wrapper at release start)
/// 2. git rev-parse --short HEAD (fallback for direct CLI usage)
pub async fn get_tag_suffix() -> Result<String> {
    // Check for RELEASE_GIT_SHA environment variable first
    // This is set by the Nix release wrapper at the START of the release,
    // ensuring we always use the code commit SHA, not any deploy commits made later
    if let Ok(sha) = env::var("RELEASE_GIT_SHA") {
        if !sha.is_empty() {
            return Ok(sha);
        }
    }

    // Fallback to git rev-parse for direct CLI usage
    let output = Command::new("git")
        .args(&["rev-parse", "--short", "HEAD"])
        .output()
        .await
        .context("Failed to execute git rev-parse - is git installed?")?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        bail!(
            "Failed to get git SHA for image tagging.\n  \
             Git error: {}\n  \
             Ensure you're in a git repository with committed changes.",
            stderr.trim()
        );
    }

    let hash = String::from_utf8_lossy(&output.stdout).trim().to_string();
    if hash.is_empty() {
        bail!("Git returned empty SHA - repository may be corrupted");
    }

    Ok(hash)
}

/// Write .version file to the service directory
/// This is a backup mechanism - the primary method is GIT_SHA environment variable
/// Returns the path where the file was written, or an error if it failed
async fn write_version_file(
    git_sha: &str,
    deploy_config: &DeployConfig,
) -> Result<std::path::PathBuf> {
    // Get the actual writable service directory in the git repository
    // Structure: {repo_root}/pkgs/products/{product}/services/rust/{service}
    let repo_root = crate::git::get_repo_root()?;
    let service_dir = repo_root
        .join(&deploy_config.global.paths.products_root)
        .join(&deploy_config.product.name)
        .join(&deploy_config.global.paths.services_path)
        .join(&deploy_config.service.name);

    let version_file = service_dir.join(".version");

    // Write the file
    tokio::fs::write(&version_file, git_sha)
        .await
        .with_context(|| {
            format!(
                "Failed to write .version file to {}",
                version_file.display()
            )
        })?;

    Ok(version_file)
}

/// Detect if cross-compilation to ARM64 is available
fn check_cross_compilation_available() -> bool {
    // Check for qemu-aarch64-static
    if which::which("qemu-aarch64-static").is_ok() {
        return true;
    }

    // Check for aarch64-linux in Nix remote builders
    if let Ok(output) = std::process::Command::new("nix")
        .args(&["show-config"])
        .output()
    {
        let config = String::from_utf8_lossy(&output.stdout);
        if config.contains("aarch64-linux") {
            return true;
        }
    }

    false
}

/// Build Rust service Docker images (AMD64 + ARM64 with cross-compilation detection)
pub async fn build_rust_service(
    service: String,
    cache_url: String,
    cache_name: String,
    _attic_token: String,
    deploy_config: &DeployConfig,
) -> Result<()> {
    println!(
        "üî® {} {} {}",
        "Building".bold(),
        service.cyan(),
        "with crate2nix (per-crate caching enabled)".dimmed()
    );
    println!("{}", "=".repeat(50));
    println!();

    // Pre-flight checks
    println!("üîç {}", "Pre-flight checks...".bold());
    check_cargo_nix_exists()?;

    // Check ATTIC_TOKEN from environment (set by Nix wrapper)
    let has_attic_token = check_token("ATTIC_TOKEN");
    if has_attic_token {
        println!("‚úÖ ATTIC_TOKEN configured");
    } else {
        println!(
            "{}",
            "‚ö†Ô∏è  Warning: ATTIC_TOKEN not set, builds will not use Attic cache".yellow()
        );
    }

    // Remove existing symlinks to avoid conflicts
    for symlink in &["result-amd64", "result-arm64"] {
        if Path::new(symlink).exists() {
            std::fs::remove_file(symlink)
                .with_context(|| format!("Failed to remove existing symlink: {}", symlink))?;
        }
    }

    // Detect cross-compilation availability
    let build_arm64 = env::var("BUILD_ARM64").unwrap_or_else(|_| "auto".to_string());
    let should_build_arm64 = match build_arm64.as_str() {
        "no" => false,
        "force" => true,
        "auto" | _ => {
            let host_arch = env::consts::ARCH;
            if host_arch == "x86_64" {
                let available = check_cross_compilation_available();
                if available {
                    println!("‚úÖ Cross-compilation to ARM64 available");
                } else {
                    println!("‚ÑπÔ∏è  Skipping ARM64 build (cross-compilation not configured)");
                    println!("   To enable: install qemu-user-static or configure remote builders");
                    println!("   Or set: export BUILD_ARM64=force");
                }
                available
            } else {
                true // On ARM hosts, can build ARM64 natively
            }
        }
    };
    println!();

    // Compute git SHA for build tagging and version embedding
    let git_sha = get_tag_suffix().await?;
    println!("üè∑Ô∏è  Git SHA: {}", git_sha);
    println!();

    // Write .version file for Nix to read (backup mechanism)
    // The primary method is GIT_SHA environment variable with --impure flag
    // This file is optional - if write fails, we continue with env var only
    println!("üìù Writing .version file...");
    match write_version_file(&git_sha, deploy_config).await {
        Ok(path) => {
            println!("   ‚úì .version file written to: {}", path.display());
        }
        Err(e) => {
            eprintln!(
                "   {}",
                format!("‚ö†Ô∏è  Warning: Could not write .version file: {}", e).yellow()
            );
            eprintln!(
                "   {}",
                "   Continuing with GIT_SHA environment variable (--impure flag)".dimmed()
            );
        }
    }
    println!();

    // Build AMD64 (always)
    println!("üì¶ {}", "Building AMD64 image...".bold());

    // Construct full cache URL with cache name (Attic serves caches at {url}/{cache-name})
    let full_cache_url = format!("{}/{}", cache_url.trim_end_matches('/'), cache_name);

    // Compute package attribute path from root flake
    // Root flake pattern: packages are named {product}-{service}
    // Example: myapp-auth, myapp-payment
    let package_attr = format!(".#{}-{}", deploy_config.product.name, service);

    // Nix Performance Optimization Strategy:
    // - max-jobs=auto: Parallelize builds across all CPU cores
    // - cores=0: Each job uses all available cores (optimal for large builds)
    // - keep-going: Don't fail entire build on first error (better for parallel builds)
    // - eval-cache: Reuse evaluation results (massive speedup on repeated builds)
    // - connect-timeout=5: Quick fallback if substituter is slow
    // - impure: Allow builtins.getEnv to read GIT_SHA environment variable
    //
    // These settings work with both regular Nix and Determinate Nix for maximum performance.
    // Your nix.conf already has good defaults, these flags ensure they're always applied.

    // Root flake pattern: Simple nix build from repo root
    // No --override-input needed, no service flake complexity
    let mut cmd = Command::new("nix");
    cmd.args(&[
        "build",
        &package_attr, // e.g., .#myapp-auth
        "--out-link",
        "result-amd64",
        "--system",
        "x86_64-linux",          // Force AMD64 build (use remote builder on Mac)
        "--impure",              // Allow reading GIT_SHA environment variable
        "--no-update-lock-file", // Use committed lock file
        // Performance: Use all available cores
        "--max-jobs",
        "auto",
        "--cores",
        "0",
        "--keep-going",
        "--eval-cache",
        // Cache configuration: Prioritize Attic cache
        "--option",
        "extra-substituters",
        &full_cache_url,
        "--option",
        "extra-trusted-public-keys",
        "cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY=",
        "--option",
        "connect-timeout",
        &deploy_config
            .global
            .deployment
            .nix_connect_timeout_secs
            .to_string(),
    ]);

    // Set GIT_SHA environment variable for Nix to read with builtins.getEnv
    // Also wrote .version file earlier as backup (belt and suspenders approach)
    cmd.env("GIT_SHA", &git_sha);

    // Configure Attic authentication via access-tokens if token is available
    if has_attic_token {
        let token = env::var("ATTIC_TOKEN").unwrap();
        // Extract hostname from full cache URL for access-tokens config
        let cache_host = full_cache_url
            .replace("https://", "")
            .replace("http://", "")
            .split('/')
            .next()
            .unwrap_or("")
            .to_string();
        let access_token_config = format!("{}={}", cache_host, token);
        cmd.args(&["--option", "access-tokens", &access_token_config]);

        // Enable attic post-build-hook for per-derivation caching
        // This automatically pushes EVERY built derivation (per-crate in crate2nix)
        // to Attic during the build, maximizing cache utilization
        //
        // Auto-discovers nix-hooks package by building .#nix-hooks
        match crate::nix_hooks::NixHooks::discover().await {
            Ok(hooks) => {
                if let Some(hook_path) = hooks.attic_push_hook_path() {
                    println!("   ‚úÖ Using attic post-build-hook: {}", hook_path.dimmed());
                    println!("   (Uploads EVERY built derivation automatically)");
                    cmd.args(&["--option", "post-build-hook", &hook_path]);

                    // Set environment variables for the hook
                    cmd.env("ATTIC_CACHE", &cache_name);
                    cmd.env("ATTIC_SERVER", &full_cache_url);
                    cmd.env("ATTIC_TOKEN", &token);
                } else {
                    println!(
                        "   {}",
                        "‚ö†Ô∏è  nix-hooks available but attic-push-hook not found".yellow()
                    );
                }
            }
            Err(e) => {
                println!(
                    "   {}",
                    format!("‚ö†Ô∏è  Could not discover nix-hooks: {}", e).yellow()
                );
                println!("   (Per-derivation caching disabled, falling back to closure push)");
            }
        }
    }

    let amd64_build = cmd
        .stdout(Stdio::inherit())
        .stderr(Stdio::inherit())
        .spawn()
        .context("Failed to spawn AMD64 build")?;

    // Build ARM64 (conditional)
    // Note: ARM64 packages not yet exposed in root flake, skip for now
    let arm64_build: Option<tokio::process::Child> = if should_build_arm64 {
        println!();
        println!("üì¶ {}", "Building ARM64 image...".bold());
        println!("   ‚ö†Ô∏è  Warning: ARM64 packages not yet exposed in root flake, skipping");
        println!("   To add ARM64 support:");
        println!(
            "   1. Add {}-{}-arm64 package to root flake.nix",
            deploy_config.product.name, service
        );
        println!("   2. Expose it in packages section");
        None

        // TODO: Uncomment when ARM64 packages are exposed in root flake
        /*
        let package_attr_arm64 = format!(".#{}-{}-arm64", deploy_config.product.name, service);

        let mut arm64_cmd = Command::new("nix");
        arm64_cmd.args(&[
            "build",
            &package_attr_arm64,
            "--out-link",
            "result-arm64",
            "--system",
            "aarch64-linux",
            "--impure",
            "--no-update-lock-file",
            "--max-jobs",
            "auto",
            "--cores",
            "0",
            "--keep-going",
            "--eval-cache",
            "--option",
            "extra-substituters",
            &full_cache_url,
            "--option",
            "extra-trusted-public-keys",
            "cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY=",
            "--option",
            "connect-timeout",
            &deploy_config.global.deployment.nix_connect_timeout_secs.to_string(),
        ]);

        arm64_cmd.env("GIT_SHA", &git_sha);

        if has_attic_token {
            let token = env::var("ATTIC_TOKEN").unwrap();
            let cache_host = full_cache_url
                .replace("https://", "")
                .replace("http://", "")
                .split('/')
                .next()
                .unwrap_or("")
                .to_string();
            let access_token_config = format!("{}={}", cache_host, token);
            arm64_cmd.args(&["--option", "access-tokens", &access_token_config]);

            if let Ok(nix_hooks_path) = env::var("NIX_HOOKS_PATH") {
                let hook_path = format!("{}/bin/attic-push-hook", nix_hooks_path);
                if std::path::Path::new(&hook_path).exists() {
                    arm64_cmd.args(&["--option", "post-build-hook", &hook_path]);
                    arm64_cmd.env("ATTIC_CACHE", &cache_name);
                    arm64_cmd.env("ATTIC_SERVER", &full_cache_url);
                    arm64_cmd.env("ATTIC_TOKEN", &token);
                }
            }
        }

        Some(
            arm64_cmd
                .stdout(Stdio::inherit())
                .stderr(Stdio::inherit())
                .spawn()
                .context("Failed to spawn ARM64 build")?,
        )
        */
    } else {
        println!();
        println!("‚è≠Ô∏è  {}", "Skipping ARM64 build".dimmed());
        None
    };

    // Wait for builds to complete
    let mut amd64_handle = amd64_build;
    let mut arm64_handle = arm64_build;

    let amd64_result = amd64_handle.wait().await?;
    let arm64_result = if let Some(mut handle) = arm64_handle {
        Some(handle.wait().await?)
    } else {
        None
    };

    // Check results
    if !amd64_result.success() {
        bail!("‚ùå AMD64 build failed!");
    }

    if let Some(arm64_status) = arm64_result {
        if !arm64_status.success() {
            bail!("‚ùå ARM64 build failed!");
        }
    }

    // Push to Attic cache - RECURSIVE CLOSURE PUSH (per-derivation caching)
    // This caches ALL Rust crate derivations, not just the final Docker image
    if has_attic_token {
        println!();
        println!(
            "üì¶ {}",
            "Pushing entire build closure to Attic cache...".bold()
        );
        println!("   This includes ALL Rust crate derivations (granular caching)");
        println!();

        let cache_target = format!("{}:{}", deploy_config.cache_server(), cache_name);

        // Push AMD64 closure recursively
        println!("   Analyzing AMD64 closure...");
        let amd64_path_info = Command::new("nix")
            .args(&["path-info", "--recursive", "result-amd64"])
            .output()
            .await
            .context("Failed to get AMD64 closure info")?;

        if !amd64_path_info.status.success() {
            eprintln!("   ‚ö†Ô∏è  Failed to get AMD64 closure info (non-fatal)");
        } else {
            let paths = String::from_utf8_lossy(&amd64_path_info.stdout);
            let closure_size = paths.lines().count();
            println!("   AMD64: Found {} derivations in closure", closure_size);

            // Push all derivations via stdin
            let mut attic_push = Command::new("attic")
                .args(&["push", &cache_target, "--stdin"])
                .stdin(std::process::Stdio::piped())
                .spawn()
                .context("Failed to spawn attic push for AMD64")?;

            if let Some(mut stdin) = attic_push.stdin.take() {
                use tokio::io::AsyncWriteExt;
                stdin.write_all(amd64_path_info.stdout.as_slice()).await?;
                drop(stdin);
            }

            let push_result = attic_push.wait().await?;

            if !push_result.success() {
                eprintln!("   ‚ö†Ô∏è  AMD64 closure push failed (non-fatal)");
            } else {
                println!("   {}", "‚úÖ AMD64 closure cached".green());
            }
        }

        // Push ARM64 closure recursively (if built)
        if should_build_arm64 {
            println!("   Analyzing ARM64 closure...");
            let arm64_path_info = Command::new("nix")
                .args(&["path-info", "--recursive", "result-arm64"])
                .output()
                .await
                .context("Failed to get ARM64 closure info")?;

            if !arm64_path_info.status.success() {
                eprintln!("   ‚ö†Ô∏è  Failed to get ARM64 closure info (non-fatal)");
            } else {
                let paths = String::from_utf8_lossy(&arm64_path_info.stdout);
                let closure_size = paths.lines().count();
                println!("   ARM64: Found {} derivations in closure", closure_size);

                // Push all derivations via stdin
                let mut attic_push = Command::new("attic")
                    .args(&["push", &cache_target, "--stdin"])
                    .stdin(std::process::Stdio::piped())
                    .spawn()
                    .context("Failed to spawn attic push for ARM64")?;

                if let Some(mut stdin) = attic_push.stdin.take() {
                    use tokio::io::AsyncWriteExt;
                    stdin.write_all(arm64_path_info.stdout.as_slice()).await?;
                    drop(stdin);
                }

                let push_result = attic_push.wait().await?;

                if !push_result.success() {
                    eprintln!("   ‚ö†Ô∏è  ARM64 closure push failed (non-fatal)");
                } else {
                    println!("   {}", "‚úÖ ARM64 closure cached".green());
                }
            }
        }

        println!();
        println!(
            "   {}",
            "‚úÖ All derivations cached in Attic (60-80% faster future builds)".green()
        );
    }

    println!();
    println!("‚úÖ {}", "Build complete!".green().bold());
    println!("   AMD64: result-amd64");
    if should_build_arm64 {
        println!("   ARM64: result-arm64");
    }
    println!("   Per-crate derivations cached in Attic for 60-80% faster future builds");

    Ok(())
}

/// Verify image exists in registry and return its digest
/// This provides a cryptographic guarantee that we're deploying exactly what we pushed
async fn verify_image_in_registry(registry: &str, full_tag_suffix: &str) -> Result<String> {
    let full_tag = format!("{}:{}", registry, full_tag_suffix);

    // Get token for authenticated registry access
    let github_token = RegistryCredentials::discover_token(None)
        .context("Registry token required for image verification")?;

    // Extract organization from registry URL for credentials
    let organization = registry.split('/').nth(1).unwrap_or("user");

    println!("   üîç Verifying image in registry: {}", full_tag.dimmed());

    let skopeo = get_tool_path("SKOPEO_BIN", "skopeo");
    let output = Command::new(&skopeo)
        .args(&[
            "inspect",
            "--creds",
            &format!("{}:{}", organization, github_token),
            "--format",
            "{{.Digest}}",
            &format!("docker://{}", full_tag),
        ])
        .output()
        .await
        .context("Failed to run skopeo inspect")?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        bail!(
            "‚ùå Image not found in registry: {}\n   \
             This could mean:\n   \
             - Push failed silently\n   \
             - Registry is temporarily unavailable\n   \
             - Authentication issue\n   \
             Error: {}",
            full_tag,
            stderr.trim()
        );
    }

    let digest = String::from_utf8_lossy(&output.stdout).trim().to_string();
    if digest.is_empty() {
        bail!("‚ùå Registry returned empty digest for {}", full_tag);
    }

    println!("   ‚úÖ Image verified: {} ({})", full_tag, &digest[..19]);
    Ok(digest)
}

/// Verify that the image in registry matches the expected digest
/// Use this before deploying to ensure we're deploying exactly what we pushed
async fn verify_image_digest_matches(
    registry: &str,
    tag: &str,
    expected_digest: &str,
) -> Result<()> {
    let current_digest = verify_image_in_registry(registry, tag).await?;

    if current_digest != expected_digest {
        bail!(
            "‚ùå Image digest mismatch!\n   \
             Expected: {}\n   \
             Found:    {}\n   \
             This could indicate a race condition or registry issue.\n   \
             Aborting deployment for safety.",
            expected_digest,
            current_digest
        );
    }

    println!("   ‚úÖ Digest verified: matches pushed image");
    Ok(())
}

/// Push docker images to the registry using the unified multi-arch strategy.
///
/// Accepts one or more (arch, path) pairs. For a single image, pushes with
/// arch-prefixed tags. For multiple images, also creates an OCI manifest index.
async fn push_docker_images(
    images: &[ArchImage],
    registry: &str,
    tag_suffix: &str,
) -> Result<()> {
    let organization = crate::infrastructure::registry::extract_organization(registry)
        .unwrap_or_else(|_| "user".to_string());

    let client = RegistryClient::discover(None, &organization)
        .context("Cannot authenticate with GHCR")?
        .with_retries(3);

    let arches: Vec<&str> = images.iter().map(|i| i.arch.as_str()).collect();
    println!(
        "üì§ Pushing {} image{} to {}...",
        arches.join(" + "),
        if images.len() > 1 { "s" } else { "" },
        registry
    );

    let result = client
        .push_multiarch(registry, images, tag_suffix)
        .await
        .context("Multi-arch push failed")?;

    println!();
    for tag in &result.arch_tags {
        println!("   ‚úÖ {}", tag);
    }
    for tag in &result.manifest_tags {
        println!("   ‚úÖ {} (manifest index)", tag);
    }

    Ok(())
}

/// Backward-compatible wrapper: push a single amd64 image
async fn push_docker_image(image_path: &str, registry: &str, tag_suffix: &str) -> Result<()> {
    push_docker_images(
        &[ArchImage {
            arch: "amd64".to_string(),
            path: image_path.to_string(),
        }],
        registry,
        tag_suffix,
    )
    .await
}

/// Push Rust service Docker image to GHCR (orchestration only, no nix build)
/// Token comes from GITHUB_TOKEN environment variable (set by substrate wrapper)
pub async fn push_rust_service(
    image_path: String,
    _service: String,
    registry: String,
    _cache_name: String,
    _attic_token: String,
    _github_token: String,
) -> Result<()> {
    let tag_suffix = get_tag_suffix().await?;
    push_docker_image(&image_path, &registry, &tag_suffix).await
}

/// Push Rust service images with explicit tag (internal implementation)
///
/// Discovers available arch images from result-{arch} symlinks, then delegates
/// to the unified push_docker_images strategy.
pub async fn push_rust_service_with_tag(
    service: String,
    registry: String,
    _cache_name: String,
    _attic_token: String,
    _github_token: String,
    tag_suffix: String,
) -> Result<()> {
    // Pre-flight checks
    if !Path::new("result-amd64").exists() {
        bail!("‚ùå Error: Build results not found\n   Run: nix run .#build");
    }

    println!(
        "üì§ {} {} {}",
        "Pushing".bold(),
        service.cyan(),
        "images to registries".dimmed()
    );
    println!("{}", "=".repeat(50));
    println!();
    println!("Registry: {}", registry);
    println!("Tag suffix: {}", tag_suffix);
    println!();

    // Collect available architecture images
    let mut images = vec![ArchImage {
        arch: "amd64".to_string(),
        path: "result-amd64".to_string(),
    }];

    if Path::new("result-arm64").exists() {
        images.push(ArchImage {
            arch: "arm64".to_string(),
            path: "result-arm64".to_string(),
        });
    }

    push_docker_images(&images, &registry, &tag_suffix).await
}

/// Resolve namespace for an environment from deploy.yaml
///
/// If namespace_override is provided, uses that.
/// Otherwise, looks up the namespace from environments.<env>.namespace in deploy.yaml.
fn resolve_namespace_for_env(env: &str, namespace_override: Option<&str>) -> Result<String> {
    if let Some(ns) = namespace_override {
        return Ok(ns.to_string());
    }

    // Read namespace from deploy.yaml based on environment
    let deploy_yaml_path = resolve_deploy_yaml_from_service_dir()?;

    if !deploy_yaml_path.exists() {
        bail!(
            "deploy.yaml not found at {}\n  \
             Required for environment-based namespace resolution",
            deploy_yaml_path.display()
        );
    }

    let content =
        std::fs::read_to_string(&deploy_yaml_path).context("Failed to read deploy.yaml")?;
    let yaml: serde_yaml::Value =
        serde_yaml::from_str(&content).context("Failed to parse deploy.yaml")?;

    // First check if there's an alias for this environment
    let resolved_env = yaml
        .get("environment_aliases")
        .and_then(|a| a.get(env))
        .and_then(|e| e.as_str())
        .unwrap_or(env);

    // Navigate to environments.<resolved_env>.namespace
    let ns = yaml
        .get("environments")
        .and_then(|e| e.get(resolved_env))
        .and_then(|e| e.get("namespace"))
        .and_then(|n| n.as_str())
        .ok_or_else(|| {
            anyhow!(
                "Namespace not found for environment '{}' in deploy.yaml\n  \
             Expected: environments.{}.namespace\n  \
             Available environments: {}",
                env,
                resolved_env,
                yaml.get("environments")
                    .and_then(|e| e.as_mapping())
                    .map(|m| m
                        .keys()
                        .filter_map(|k| k.as_str())
                        .collect::<Vec<_>>()
                        .join(", "))
                    .unwrap_or_else(|| "none".to_string())
            )
        })?;

    Ok(ns.to_string())
}

/// Get manifest path for an environment from deploy.yaml
fn get_manifest_path_for_env(env: &str) -> Result<String> {
    let deploy_yaml_path = resolve_deploy_yaml_from_service_dir()?;

    if !deploy_yaml_path.exists() {
        bail!(
            "deploy.yaml not found at {}\n  \
             Required for manifest path lookup",
            deploy_yaml_path.display()
        );
    }

    let content =
        std::fs::read_to_string(&deploy_yaml_path).context("Failed to read deploy.yaml")?;
    let yaml: serde_yaml::Value =
        serde_yaml::from_str(&content).context("Failed to parse deploy.yaml")?;

    // First check if there's an alias for this environment
    let resolved_env = yaml
        .get("environment_aliases")
        .and_then(|a| a.get(env))
        .and_then(|e| e.as_str())
        .unwrap_or(env);

    // Navigate to manifests.<resolved_env>.kustomization
    let manifest = yaml
        .get("manifests")
        .and_then(|m| m.get(resolved_env))
        .and_then(|m| m.get("kustomization"))
        .and_then(|k| k.as_str())
        .ok_or_else(|| {
            anyhow!(
                "Manifest path not found for environment '{}' in deploy.yaml\n  \
             Expected: manifests.{}.kustomization\n  \
             Available manifests: {}",
                env,
                resolved_env,
                yaml.get("manifests")
                    .and_then(|m| m.as_mapping())
                    .map(|m| m
                        .keys()
                        .filter_map(|k| k.as_str())
                        .collect::<Vec<_>>()
                        .join(", "))
                    .unwrap_or_else(|| "none".to_string())
            )
        })?;

    Ok(manifest.to_string())
}

/// Deploy to a single environment (used by orchestrate_release for each env)
///
/// This handles:
/// - Updating the kustomization manifest
/// - Committing the changes
/// - Triggering flux reconcile
///
/// When `k8s_repo_root` is Some, manifest paths are resolved and git operations
/// happen relative to that directory (separate k8s repo).
async fn deploy_to_environment(
    service: &str,
    env: &str,
    namespace: &str,
    registry: &str,
    tag_suffix: &str,
    watch: bool,
    k8s_repo_root: Option<&std::path::Path>,
    k8s_branch: Option<&str>,
) -> Result<String> {
    let manifest = get_manifest_path_for_env(env)?;

    // If k8s repo is configured, resolve manifest relative to it
    let full_manifest_path = if let Some(k8s_root) = k8s_repo_root {
        k8s_root.join(&manifest)
    } else {
        let repo_root = crate::git::get_repo_root()?;
        repo_root.join(&manifest)
    };

    println!("   üìÅ Manifest: {}", manifest.dimmed());

    // Deploy via GitOps (updates manifest, commits)
    let git_sha = deploy_rust_service_with_tag(
        service.to_string(),
        full_manifest_path.to_string_lossy().to_string(),
        registry.to_string(),
        namespace.to_string(),
        watch,
        tag_suffix.to_string(),
        k8s_repo_root.map(|p| p.to_path_buf()),
        k8s_branch.map(|s| s.to_string()),
    )
    .await?;

    Ok(git_sha)
}

/// Full orchestration release workflow (orchestration only, no nix build)
/// This is the main entry point for release workflows from substrate wrappers
///
/// Environment selection:
/// - Uses `environment` parameter (from --environment flag or FORGE_ENV env var)
/// - If `single_environment` is false (default), deploys to ALL environments from deploy.yaml
/// - Reads namespace from service deploy.yaml `environments.<env>.namespace`
/// - If `namespace` is provided explicitly, it overrides the deploy.yaml lookup
pub async fn orchestrate_release(
    service: String,
    registry: String,
    environment: String,
    single_environment: bool,
    namespace_override: Option<String>,
    image_path: Option<String>,
    image_path_arm64: Option<String>,
    watch: bool,
    push_only: bool,
    deploy_only: bool,
    image_tag: Option<String>,
) -> Result<()> {
    // Validate flag combinations
    if push_only && deploy_only {
        bail!("Cannot use both --push-only and --deploy-only");
    }

    // Load deployment configuration first (hierarchical: global ‚Üí product ‚Üí service)
    let deploy_config = DeployConfig::load_for_service(&service)?;

    // Determine which environments to deploy to
    // Respects active_environments filter from deploy.yaml
    let environments: Vec<String> = if single_environment {
        // Single environment mode: deploy only to the specified environment
        // Still checks if it's in active_environments
        deploy_config.service.release.get_environments(&environment)
    } else {
        // Multi-environment mode (default): deploy to all ACTIVE environments in order
        deploy_config.service.release.get_environments("all")
    };

    // Fail fast if no environments to deploy to (unless push-only)
    if environments.is_empty() && !push_only {
        bail!(
            "‚ùå No active environments to deploy to.\n   \
             Check deploy.yaml active_environments configuration.\n   \
             Available environments: {:?}\n   \
             Active environments: {:?}",
            deploy_config.service.release.environment_order,
            deploy_config.service.release.active_environments
        );
    }

    let mode_label = if push_only {
        "Push-Only"
    } else if deploy_only {
        "Deploy-Only"
    } else {
        "Build-Once-Promote Release"
    };

    println!(
        "üöÄ {} {} {}",
        service.cyan().bold(),
        mode_label.bold(),
        format!(
            "({} environment{})",
            environments.len(),
            if environments.len() == 1 { "" } else { "s" }
        )
        .dimmed()
    );
    println!("{}", "=".repeat(60));
    println!();

    // Show active vs available environments (skip for push-only)
    if !push_only {
        let all_envs = &deploy_config.service.release.environment_order;
        let active_envs = deploy_config.service.release.effective_environments();
        if all_envs.len() != active_envs.len() {
            println!("üìç Environment Status:");
            for env in all_envs {
                if active_envs.contains(env) {
                    println!("   {} {} (active)", "‚óè".green(), env.cyan());
                } else {
                    println!("   {} {} (inactive)", "‚óã".dimmed(), env.dimmed());
                }
            }
            println!();
        }

        // Show deployment plan
        println!("üìã Deployment Plan:");
        for (i, env) in environments.iter().enumerate() {
            let namespace = resolve_namespace_for_env(env, namespace_override.as_deref())?;
            println!("   {}. {} ‚Üí {}", i + 1, env.cyan(), namespace.dimmed());
        }
        println!();
    }

    // Use the first environment for initial setup (if available)
    let namespace = if !environments.is_empty() {
        let first_env = &environments[0];
        resolve_namespace_for_env(first_env, namespace_override.as_deref())?
    } else {
        String::new()
    };

    // CRITICAL: Prevent concurrent releases of the same service
    // Check if a release lock file exists for this service
    let lock_file = format!("/tmp/forge-{}.lock", service);
    if std::path::Path::new(&lock_file).exists() {
        // Try to read the PID from the lock file
        if let Ok(contents) = std::fs::read_to_string(&lock_file) {
            if let Ok(pid) = contents.trim().parse::<i32>() {
                // Check if the process is still running
                use std::process::Command as StdCommand;
                let check_result = StdCommand::new("ps")
                    .args(&["-p", &pid.to_string()])
                    .output();

                if let Ok(output) = check_result {
                    if output.status.success()
                        && !String::from_utf8_lossy(&output.stdout).is_empty()
                    {
                        bail!(
                            "‚ùå Another release for '{}' is already running (PID: {})\n\
                             Wait for it to complete or kill it with: kill {}",
                            service,
                            pid,
                            pid
                        );
                    }
                }
            }
        }
    }

    // Create lock file with current PID
    let current_pid = std::process::id();
    std::fs::write(&lock_file, current_pid.to_string())
        .context("Failed to create release lock file")?;

    // Ensure lock file is cleaned up on exit
    let lock_file_cleanup = lock_file.clone();
    let _guard = scopeguard::guard((), move |_| {
        let _ = std::fs::remove_file(&lock_file_cleanup);
    });

    // Step 0: Pre-release FluxCD health check (can be skipped via config)
    let skip_flux_health_check = deploy_config
        .service
        .deployment
        .as_ref()
        .map(|d| d.skip_flux_health_check)
        .unwrap_or(deploy_config.global.deployment.skip_flux_health_check);

    if !deploy_only {
        if skip_flux_health_check {
            println!(
                "Step 0: {}",
                "Skipping FluxCD health check (skip_flux_health_check: true)"
                    .bold()
                    .dimmed()
            );
        } else {
            println!("Step 0: {}", "Pre-release FluxCD health check...".bold());
            crate::commands::flux::health_check("pre-release").await?;
        }
        println!();
    }

    // Create service configuration from deploy.yaml
    let config = ServiceConfig::from_config(service.clone(), &deploy_config);

    // Resolve k8s repo root if configured (for multi-repo deployments)
    let repo_root = crate::git::get_repo_root()?;
    let product_dir = crate::config::resolve_product_dir(&repo_root, &deploy_config.product.name);
    let k8s_repo_root = if deploy_config.product.k8s.is_some() {
        Some(crate::config::resolve_k8s_repo_root(&deploy_config.product, &product_dir))
    } else {
        None
    };
    let k8s_branch = deploy_config
        .product
        .k8s
        .as_ref()
        .map(|k| k.branch.clone());

    // Determine the service directory for pre-deployment tests
    let pre_deploy_service_dir = if service == "web" {
        repo_root
            .join(&deploy_config.global.paths.products_root)
            .join(&deploy_config.product.name)
            .join("web")
    } else {
        repo_root
            .join(&deploy_config.global.paths.products_root)
            .join(&deploy_config.product.name)
            .join(&deploy_config.global.paths.services_path)
            .join(&service)
    };

    // Step 0.5: Run pre-deployment tests (BEFORE push/deploy)
    // Skip if deploy-only (tests already ran during push phase)
    if !deploy_only {
        let pre_deploy_config = deploy_config
            .service
            .deployment
            .as_ref()
            .map(|d| &d.pre_deployment_tests)
            .unwrap_or(&deploy_config.global.deployment.pre_deployment_tests);

        if pre_deploy_config.enabled {
            println!("Step 0.5: {}", "Running pre-deployment tests...".bold());
            crate::commands::integration_tests::execute_pre_deployment_tests(
                pre_deploy_config,
                pre_deploy_service_dir.clone(),
                &service,
            )
            .await?;
            println!();
        }
    }

    // Tag resolution:
    // - deploy_only with image_tag: use the provided tag
    // - otherwise: get_tag_suffix() (RELEASE_GIT_SHA or git rev-parse)
    let tag_suffix = if deploy_only {
        image_tag
            .clone()
            .context("--image-tag required with --deploy-only")?
    } else {
        get_tag_suffix().await?
    };
    let has_arm64 = image_path_arm64.is_some();
    if has_arm64 {
        println!("üè∑Ô∏è  Image tags: amd64-{}, arm64-{}, {} (manifest)", tag_suffix, tag_suffix, tag_suffix);
    } else {
        println!("üè∑Ô∏è  Image tag: amd64-{}", tag_suffix);
    }
    println!();

    // Step 1: Push (skip if deploy-only)
    if !deploy_only {
        let img_path = image_path
            .as_ref()
            .context("--image-path required for push")?;

        // Collect all architecture images
        let mut images = vec![ArchImage {
            arch: "amd64".to_string(),
            path: img_path.clone(),
        }];

        if let Some(arm64_path) = &image_path_arm64 {
            images.push(ArchImage {
                arch: "arm64".to_string(),
                path: arm64_path.clone(),
            });
        }

        println!("Step 1: {}", "Pushing to GHCR...".bold());
        push_docker_images(&images, &registry, &tag_suffix).await?;

        // Step 1.5: Verify amd64 image exists in registry and capture digest
        println!();
        println!("Step 1.5: {}", "Verifying image in registry...".bold());
        let verify_tag = format!("amd64-{}", tag_suffix);
        let pushed_digest = verify_image_in_registry(&registry, &verify_tag).await?;
        println!("   üìã Captured digest: {}", pushed_digest);
        println!();

        // If push-only, we're done
        if push_only {
            println!("{}", "‚îÅ".repeat(60).bright_green());
            if has_arm64 {
                println!(
                    "{}  Tags: amd64-{}, arm64-{}, {} (manifest)",
                    "PUSH COMPLETE".green().bold(),
                    tag_suffix,
                    tag_suffix,
                    tag_suffix
                );
            } else {
                println!(
                    "{}  Tag: amd64-{}",
                    "PUSH COMPLETE".green().bold(),
                    tag_suffix
                );
            }
            println!("{}", "‚îÅ".repeat(60).bright_green());
            return Ok(());
        }

        // Step 2.5: Verify image digest hasn't changed (race condition protection)
        // before deploying
        verify_image_digest_matches(&registry, &verify_tag, &pushed_digest).await?;
    }

    // Step 2: Run migrations BEFORE deployment (using pushed image)
    // CRITICAL: Migrations must complete successfully BEFORE updating K8s manifests
    // This ensures the database schema is ready when new pods start
    println!("Step 2: {}", "Running database migrations...".bold());

    if config.database_type() == &crate::commands::service_config::DatabaseType::None {
        println!(
            "   {} {}",
            "‚Ü∑".dimmed(),
            "Skipping migrations (database_type: none)".dimmed()
        );
    } else {
        // For multi-environment deployments, run migrations for each environment in order
        // Each environment may have its own database
        for (i, env) in environments.iter().enumerate() {
            let namespace = resolve_namespace_for_env(env, namespace_override.as_deref())?;

            println!();
            println!(
                "   [{}/{}] {} migrations ‚Üí {}",
                i + 1,
                environments.len(),
                env.cyan().bold(),
                namespace.dimmed()
            );

            // Check and reset stuck Shinka migrations first
            if let Ok(was_reset) = crate::commands::migrations::check_and_reset_shinka_migration(
                &deploy_config.product.name,
                &service,
                &namespace,
            )
            .await
            {
                if was_reset {
                    println!(
                        "   {} Shinka migration reset, will retry with new image",
                        "‚úÖ".green()
                    );
                }
            }

            // Run migrations for this environment
            let migration_image_tag = format!("amd64-{}", tag_suffix);
            crate::commands::migrations::run_migrations(
                &config,
                namespace.clone(),
                migration_image_tag.clone(),
                &deploy_config,
            )
            .await?;
            println!(
                "   {} Migrations completed for {}",
                "‚úÖ".green(),
                env.cyan()
            );
        }
    }
    println!();

    // Step 3: Deploy to each environment in order (AFTER migrations pass)
    // Now that the database schema is ready, it's safe to deploy new pods
    println!("Step 3: {}", "Deploying to environments...".bold());
    let mut last_git_sha = String::new();
    let mut last_namespace = String::new();

    for (i, env) in environments.iter().enumerate() {
        let namespace = resolve_namespace_for_env(env, namespace_override.as_deref())?;

        println!();
        println!(
            "   [{}/{}] {} ‚Üí {}",
            i + 1,
            environments.len(),
            env.cyan().bold(),
            namespace.dimmed()
        );

        // Deploy to this environment (updates manifest, commits)
        last_git_sha =
            deploy_to_environment(
                &service, env, &namespace, &registry, &tag_suffix, watch,
                k8s_repo_root.as_deref(),
                k8s_branch.as_deref(),
            ).await?;
        last_namespace = namespace.clone();

        // Trigger flux reconcile for this namespace (non-blocking unless it's the last env)
        let is_last_env = i == environments.len() - 1;
        if !is_last_env || !deploy_config.service.release.wait_between_environments {
            // Just trigger reconcile, don't wait
            println!("   ‚ö° Triggering flux reconcile for {}", namespace.cyan());
            crate::commands::flux::reconcile(namespace.clone()).await?;
        }

        // Shinka migration coordination:
        // - shinka_gating=true: Block release until migration completes (legacy)
        // - shinka_gating=false: Set expected-tag annotation and continue (recommended)
        //   The K8s layer handles coordination via Shinka + wait-for-migrations init container
        if deploy_config.service.migration.shinka_gating {
            let expected_image_tag = format!("amd64-{}", tag_suffix);
            crate::commands::migrations::wait_for_shinka_migration(
                &deploy_config.product.name,
                &service,
                &namespace,
                &expected_image_tag,
                deploy_config
                    .service
                    .migration
                    .shinka_migration_name
                    .as_deref(),
                deploy_config.service.migration.shinka_timeout_secs,
            )
            .await?;
        } else {
            // Set expected-tag annotation so Shinka picks up the new release faster
            // (invalidates cache, fast-requeues, auto-retries from Failed state)
            let migration_name = deploy_config
                .service
                .migration
                .shinka_migration_name
                .clone()
                .unwrap_or_else(|| {
                    format!("{}-{}", deploy_config.product.name, service)
                });
            let expected_image_tag = format!("amd64-{}", tag_suffix);
            crate::commands::migrations::set_expected_tag_if_exists(
                &migration_name,
                &namespace,
                &expected_image_tag,
            )
            .await;
        }
    }
    println!();

    // For subsequent steps, use the last environment's namespace
    let namespace = last_namespace;
    let git_sha = last_git_sha;

    // Step 4: Wait for deployment to be ready
    let wait_for_rollout = deploy_config
        .service
        .deployment
        .as_ref()
        .map(|d| d.wait_for_rollout)
        .unwrap_or(deploy_config.global.deployment.wait_for_rollout);

    if wait_for_rollout {
        println!("Step 4: {}", "Waiting for deployment rollout...".bold());
        crate::commands::flux::wait_for_deployment(
            service.clone(),
            namespace.clone(),
            deploy_config.global.deployment.deployment_wait_timeout_secs,
            tag_suffix.clone(),
            &deploy_config,
        )
        .await?;
        println!();
    } else {
        println!(
            "Step 4: {}",
            "Skipping deployment rollout wait (wait_for_rollout: false)".dimmed()
        );
        println!();
    }

    // Step 4.5 (conditional): Search service GitOps sync
    // Only runs for services with novasearch.enabled = true in deploy.yaml
    if crate::commands::search_sync::should_run_novasearch_sync(&deploy_config) {
        println!("Step 4.5: {}", "Running search service sync...".bold());
        let service_dir = std::path::Path::new(".");
        crate::commands::search_sync::run_novasearch_sync(service_dir, &namespace, &deploy_config)
            .await?;
        println!();
    }

    // Step 5: Extract GraphQL schema
    println!("Step 5: {}", "Extracting GraphQL schema...".bold());
    crate::commands::federation::extract_schema(service.clone(), &deploy_config).await?;
    println!();

    // Step 6: Update GraphQL Federation (Hive Router)
    println!("Step 6: {}", "Updating Hive Router federation...".bold());
    crate::commands::federation::update_federation(
        service.clone(),
        namespace.clone(),
        &deploy_config,
    )
    .await?;
    println!();

    // Step 7: Post-release FluxCD health check with retry (can be skipped via config)
    // CRITICAL: Verify GitOps system is still healthy after deployment
    // Wait for Flux to finish reconciling changes before declaring success
    // This ensures the release didn't break the cluster reconciliation
    if skip_flux_health_check {
        println!(
            "Step 7: {}",
            "Skipping post-release FluxCD health check (skip_flux_health_check: true)".dimmed()
        );
    } else {
        println!("Step 7: {}", "Post-release FluxCD health check...".bold());
        // Wait up to 10 minutes for all kustomizations to reconcile (was 5 minutes)
        // Longer timeout handles complex deployments with multiple services
        crate::commands::flux::health_check_with_retry("post-release", 600, 10).await?;
    }
    println!();

    // Step 8: Run integration tests (if configured in deploy.yaml)
    // CRITICAL: Tests run AFTER Hive Router is updated and FluxCD has reconciled
    // This ensures we're testing against the latest federated schema
    let repo_root = crate::git::get_repo_root()?;
    let service_dir = if service == "web" {
        // Web service: pkgs/products/{product}/web
        repo_root
            .join(&deploy_config.global.paths.products_root)
            .join(&deploy_config.product.name)
            .join("web")
    } else {
        // Rust service: pkgs/products/{product}/services/rust/{service}
        repo_root
            .join(&deploy_config.global.paths.products_root)
            .join(&deploy_config.product.name)
            .join(&deploy_config.global.paths.services_path)
            .join(&service)
    };

    let product_dir = repo_root
        .join(&deploy_config.global.paths.products_root)
        .join(&deploy_config.product.name);
    let deploy_yaml_path = resolve_deploy_yaml_path(&product_dir, &service, &service_dir);
    if deploy_yaml_path.exists() {
        // Try to load integration test config from deploy.yaml
        if let Ok(yaml_content) = tokio::fs::read_to_string(&deploy_yaml_path).await {
            if let Ok(yaml_value) = serde_yaml::from_str::<serde_yaml::Value>(&yaml_content) {
                // Check if deployment.integration_tests exists and is enabled
                if let Some(deployment) = yaml_value.get("deployment") {
                    if let Some(integration_tests) = deployment.get("integration_tests") {
                        if let Some(enabled) = integration_tests.get("enabled") {
                            if enabled.as_bool().unwrap_or(false) {
                                // Parse the integration_tests config
                                if let Ok(config) =
                                    serde_yaml::from_value::<
                                        crate::commands::integration_tests::IntegrationTestConfig,
                                    >(integration_tests.clone())
                                {
                                    println!(
                                        "Step 8: {}",
                                        "Running post-deployment integration tests...".bold()
                                    );
                                    println!("‚ÑπÔ∏è  Testing against stable Hive Router with updated federation schema");
                                    println!();
                                    match crate::commands::integration_tests::execute(
                                        config,
                                        service_dir.clone(),
                                    )
                                    .await
                                    {
                                        Ok(_) => {
                                            println!("‚úÖ Integration tests passed!");
                                            println!();
                                        }
                                        Err(e) => {
                                            println!();
                                            println!(
                                                "{}",
                                                "‚úó Integration tests failed".red().bold()
                                            );
                                            println!();
                                            return Err(e);
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    println!("{}", "‚îÅ".repeat(80).bright_green());
    println!(
        "{}",
        "‚úÖ RELEASE COMPLETE - ALL SYSTEMS HEALTHY".green().bold()
    );
    println!("{}", "‚îÅ".repeat(80).bright_green());

    Ok(())
}

/// Deploy Rust service to Kubernetes via GitOps
///
/// Returns the git SHA suffix used in the deployment
/// Deploy Rust service - wrapper that gets tag internally
pub async fn deploy_rust_service(
    service: String,
    manifest: String,
    registry: String,
    namespace: String,
    watch: bool,
) -> Result<String> {
    let tag_suffix = get_tag_suffix().await?;
    deploy_rust_service_with_tag(service, manifest, registry, namespace, watch, tag_suffix, None, None).await
}

/// Update a kustomization image tag using targeted text replacement.
///
/// Instead of round-tripping through serde_yaml (which destroys comments,
/// reformats multi-line strings, and can corrupt patch: | blocks), this
/// function finds the image entry matching `service_name` and replaces
/// only the `newTag` value.
///
/// Expected YAML structure:
/// ```yaml
/// images:
/// - name: ghcr.io/your-org/your-project/my-backend
///   newTag: amd64-abc123
/// ```
fn update_kustomization_image_tag(
    content: &str,
    service_name: &str,
    new_tag: &str,
) -> Result<String> {
    let lines: Vec<&str> = content.lines().collect();
    let mut result = Vec::with_capacity(lines.len());
    let mut found = false;
    let mut matched_name = false;

    for (i, line) in lines.iter().enumerate() {
        if matched_name {
            // The previous line was a matching `- name:` entry.
            // The next non-empty line should be `  newTag: ...`
            let trimmed = line.trim();
            if trimmed.starts_with("newTag:") {
                // Replace the newTag value, preserving indentation
                let indent = &line[..line.len() - line.trim_start().len()];
                result.push(format!("{}newTag: {}", indent, new_tag));
                found = true;
                matched_name = false;
                continue;
            }
            // If it's not newTag, keep looking (might be a comment or blank line)
            if !trimmed.is_empty() && !trimmed.starts_with('#') {
                matched_name = false;
            }
        }

        // Check if this line is `- name: <something containing service_name>`
        let trimmed = line.trim();
        if trimmed.starts_with("- name:") {
            let name_value = trimmed.trim_start_matches("- name:").trim();
            if name_value.contains(service_name) {
                matched_name = true;
            }
        }

        result.push(line.to_string());
    }

    if !found {
        bail!(
            "No image entry matching '{}' found in kustomization manifest",
            service_name
        );
    }

    // Preserve trailing newline if original had one
    let mut output = result.join("\n");
    if content.ends_with('\n') {
        output.push('\n');
    }

    Ok(output)
}

/// Deploy Rust service with explicit tag (internal implementation)
///
/// When `k8s_workdir` is Some, git operations (add/commit/push) happen in that
/// directory instead of the product repo root (for separate k8s repos).
pub async fn deploy_rust_service_with_tag(
    service: String,
    manifest: String,
    registry: String,
    namespace: String,
    watch: bool,
    tag_suffix: String,
    k8s_workdir: Option<std::path::PathBuf>,
    k8s_branch: Option<String>,
) -> Result<String> {
    let image_tag = format!("{}:amd64-{}", registry, tag_suffix);

    println!(
        "üéØ {} {} {}",
        "Deploying".bold(),
        service.cyan(),
        "to Kubernetes".dimmed()
    );
    println!("{}", "=".repeat(50));
    println!();
    println!("Namespace: {}", namespace);
    println!("Image: {}", image_tag);
    println!("Manifest: {}", manifest);
    println!();

    // Verify image exists in registry before updating kustomization.
    // This prevents deploying non-existent images (ImagePullBackOff).
    match verify_image_in_registry(&registry, &tag_suffix).await {
        Ok(_digest) => {
            println!("   ‚úÖ Image verified in registry");
        }
        Err(e) => {
            // Check if this is just a missing token (non-interactive environment)
            let err_str = format!("{}", e);
            if err_str.contains("GITHUB_TOKEN") || err_str.contains("GHCR_TOKEN") {
                eprintln!(
                    "   {} Skipping image verification (no registry credentials available)",
                    "‚ö†Ô∏è".yellow()
                );
            } else {
                bail!(
                    "‚ùå Image {}:amd64-{} does not exist in registry.\n   \
                     Cannot deploy a non-existent image.\n   \
                     Error: {}",
                    registry,
                    tag_suffix,
                    e
                );
            }
        }
    }
    println!();

    // Read manifest and update image tag using targeted text replacement.
    // CRITICAL: Do NOT round-trip through serde_yaml - it destroys comments,
    // reformats multi-line strings (patch: | blocks), and can corrupt the file.
    let manifest_content = tokio::fs::read_to_string(&manifest)
        .await
        .context("Failed to read manifest")?;

    let new_tag = format!("amd64-{}", tag_suffix);
    let updated_manifest =
        update_kustomization_image_tag(&manifest_content, &service, &new_tag)?;

    tokio::fs::write(&manifest, &updated_manifest)
        .await
        .context("Failed to write updated manifest")?;

    // Git commit and push (in k8s repo if configured, otherwise product repo)
    let git_workdir = k8s_workdir.as_deref();
    let git_branch = k8s_branch.as_deref().unwrap_or("main");

    if let Some(workdir) = git_workdir {
        // Multi-repo: git operations in the k8s repo
        let manifest_path = std::path::Path::new(&manifest);
        crate::git::commit_and_push_in(
            workdir,
            &[manifest_path],
            &format!("Deploy {} {}", service, tag_suffix),
            git_branch,
        )?;
    } else {
        // Single-repo: git operations in current repo
        Command::new("git")
            .args(&["add", &manifest])
            .status()
            .await
            .context("Failed to stage manifest")?;

        Command::new("git")
            .args(&[
                "commit",
                "-m",
                &format!("Deploy {} {}", service, tag_suffix),
            ])
            .stdout(Stdio::inherit())
            .stderr(Stdio::inherit())
            .status()
            .await
            .context("Failed to commit manifest")?;

        Command::new("git")
            .args(&["push", "origin", git_branch])
            .stdout(Stdio::inherit())
            .stderr(Stdio::inherit())
            .status()
            .await
            .context("Failed to push manifest")?;
    }

    println!("‚úÖ {}", "Manifest updated and pushed".green());

    if watch {
        println!();
        println!(
            "‚ÑπÔ∏è  Flux will handle deployment - use 'kubectl get pods -n {}' to monitor",
            namespace
        );
    }

    println!();
    println!("‚úÖ {}", "GitOps deployment triggered!".green().bold());

    Ok(tag_suffix)
}

/// Print comprehensive deployment report
async fn print_deployment_report(
    service: &str,
    namespace: &str,
    deploy_config: &DeployConfig,
    tag_suffix: &str,
) -> Result<()> {
    use colored::Colorize;

    println!();
    println!("{} {}", "Service:".bold(), service.cyan());
    println!("{} {}", "Namespace:".bold(), namespace);
    println!(
        "{} {}",
        "Environment:".bold(),
        deploy_config.product.environment
    );
    println!("{} {}", "Image Tag:".bold(), tag_suffix.yellow());
    println!();

    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    // COMPLETED ACTIONS
    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    println!("{}", "‚úÖ COMPLETED ACTIONS".green().bold());
    println!("{}", "‚îÄ".repeat(80).dimmed());
    println!();

    println!("  {} Docker Image", "‚úì".green());
    println!("    ‚Ä¢ Built with crate2nix (per-crate Attic caching)");
    println!("    ‚Ä¢ Pushed to GHCR: {}", deploy_config.registry_url());
    println!("    ‚Ä¢ Tags: amd64-latest, amd64-{}", tag_suffix);
    println!();

    println!("  {} GitOps Deployment", "‚úì".green());
    println!("    ‚Ä¢ Manifest updated with new image tag");
    println!("    ‚Ä¢ Changes committed to git (main branch)");
    println!("    ‚Ä¢ Pushed to remote repository");
    println!();

    println!("  {} Database Migrations", "‚úì".green());
    println!("    ‚Ä¢ Migration job completed successfully");
    println!("    ‚Ä¢ Schema is up to date");
    println!();

    println!("  {} GraphQL Federation", "‚úì".green());
    println!("    ‚Ä¢ Schema extracted from Rust code");
    println!("    ‚Ä¢ Supergraph composed with Rover (Federation v2.11.3)");
    println!("    ‚Ä¢ Hive Router deployment updated with hash");
    println!("    ‚Ä¢ Federation changes committed and pushed");
    println!();

    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    // PENDING ROLLOUTS (FluxCD-managed)
    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    println!("{}", "‚è≥ PENDING ROLLOUTS".yellow().bold());
    println!("{}", "‚îÄ".repeat(80).dimmed());
    println!();

    // Check current pod status
    let pod_status = Command::new("kubectl")
        .args(&[
            "get",
            "pods",
            "-n",
            namespace,
            "-l",
            &format!("app={}", service),
            "-o",
            "jsonpath={.items[0].status.phase}",
        ])
        .output()
        .await;

    let current_image = Command::new("kubectl")
        .args(&[
            "get",
            "pods",
            "-n",
            namespace,
            "-l",
            &format!("app={}", service),
            "-o",
            "jsonpath={.items[0].spec.containers[0].image}",
        ])
        .output()
        .await;

    if let (Ok(status_output), Ok(image_output)) = (&pod_status, &current_image) {
        if status_output.status.success() && image_output.status.success() {
            let phase = String::from_utf8_lossy(&status_output.stdout);
            let image = String::from_utf8_lossy(&image_output.stdout);

            println!("  {} Service Pod Rollout", "‚è≥".yellow());
            println!("    ‚Ä¢ Current pod status: {}", phase);
            println!("    ‚Ä¢ Current image: {}", image.dimmed());

            let expected_image = format!("{}:amd64-{}", deploy_config.registry_url(), tag_suffix);
            if image.contains(tag_suffix) {
                println!("    ‚Ä¢ {} New image is already deployed!", "‚úì".green());
            } else {
                println!("    ‚Ä¢ ‚è≥ Waiting for Flux to deploy new image...");
            }
        } else {
            println!("  {} Service Pod Rollout", "‚è≥".yellow());
            println!("    ‚Ä¢ Flux will create/update pod with new image");
        }
    } else {
        println!("  {} Service Pod Rollout", "‚è≥".yellow());
        println!("    ‚Ä¢ Flux will deploy the new pod (typically 30-60 seconds)");
    }
    println!();

    println!("  {} Hive Router Update", "‚è≥".yellow());
    println!("    ‚Ä¢ Flux will detect supergraph.graphql changes");
    println!("    ‚Ä¢ New ConfigMap will be generated (hash suffix)");
    println!("    ‚Ä¢ Hive Router pod will restart automatically");
    println!("    ‚Ä¢ New schema will be live after restart (typically 1-2 minutes)");
    println!();

    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    // VERIFICATION COMMANDS
    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    println!("{}", "‚úì VERIFICATION COMMANDS".green().bold());
    println!("{}", "‚îÄ".repeat(80).dimmed());
    println!();

    println!("  Verify Docker image exists in registry:");
    println!(
        "  $ {}",
        format!(
            "docker pull {}:amd64-{}",
            deploy_config.registry_url(),
            tag_suffix
        )
        .yellow()
    );
    println!();

    println!("  Verify git commits were pushed:");
    println!("  $ {}", "git log --oneline -5".yellow());
    println!();

    println!("  Verify kustomization manifest has new image tag:");
    println!(
        "  $ {}",
        format!(
            "grep {} nix/k8s/clusters/{{cluster}}/products/{}/services/{}/kustomization.yaml",
            tag_suffix, namespace, service
        )
        .yellow()
    );
    println!();

    println!("  Verify new pod is running with correct image:");
    println!(
        "  $ {}",
        format!(
            "kubectl get pods -n {} -l app={} -o jsonpath='{{.items[0].spec.containers[0].image}}'",
            namespace, service
        )
        .yellow()
    );
    println!(
        "    Expected: {}:amd64-{}",
        deploy_config.registry_url(),
        tag_suffix
    );
    println!();

    println!("  Verify database migrations completed:");
    println!(
        "  $ {}",
        format!(
            "kubectl get jobs -n {} | grep {}-migration",
            namespace, service
        )
        .yellow()
    );
    println!("    Should show: Completed (1/1)");
    println!();

    println!("  Verify GraphQL schema file updated:");
    println!("  $ {}", "git log --oneline --all -- pkgs/products/<product>/infrastructure/hive-router/subgraphs/*.graphql | head -1".yellow());
    println!("    Should show recent commit with schema update");
    println!();

    println!("  Verify supergraph.graphql updated:");
    println!("  $ {}", format!("grep 'graph: {}' nix/k8s/clusters/{{cluster}}/products/{}/hive-router/supergraph.graphql | wc -l",
        service.to_uppercase(), namespace).yellow());
    println!("    Should be > 0 (service present in supergraph)");
    println!();

    println!("  Verify Hive Router has new supergraph hash:");
    println!("  $ {}", format!("kubectl get deployment hive-router -n {} -o jsonpath='{{.spec.template.metadata.annotations.supergraph\\.hash}}'",
        namespace).yellow());
    println!("    Check if hash is recent");
    println!();

    println!("  Verify Hive Router ConfigMap updated:");
    println!(
        "  $ {}",
        format!(
            "kubectl get configmap -n {} | grep hive-router-config",
            namespace
        )
        .yellow()
    );
    println!("    Hash suffix should have changed (triggers pod restart)");
    println!();

    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    // MONITORING COMMANDS
    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    println!("{}", "üîç MONITORING COMMANDS".cyan().bold());
    println!("{}", "‚îÄ".repeat(80).dimmed());
    println!();

    println!("  Watch service pod rollout:");
    println!(
        "  $ {}",
        format!("kubectl get pods -n {} -w | grep {}", namespace, service).yellow()
    );
    println!();

    println!("  Check service pod logs:");
    println!(
        "  $ {}",
        format!("kubectl logs -n {} -l app={} -f", namespace, service).yellow()
    );
    println!();

    println!("  Check Hive Router status:");
    println!(
        "  $ {}",
        format!("kubectl get pods -n {} | grep hive-router", namespace).yellow()
    );
    println!();

    println!("  Check Hive Router logs:");
    println!(
        "  $ {}",
        format!("kubectl logs -n {} -l app=hive-router -f", namespace).yellow()
    );
    println!();

    println!("  Check Flux reconciliation status:");
    println!("  $ {}", "flux get kustomizations".yellow());
    println!();

    println!("  Test GraphQL endpoint (after router updates):");
    println!(
        "  $ {}",
        format!(
            "kubectl port-forward -n {} svc/hive-router 4000:4000",
            namespace
        )
        .yellow()
    );
    println!(
        "  $ {}",
        "curl http://localhost:4000/graphql -d '{\"query\":\"{__schema{types{name}}}\"}'".yellow()
    );
    println!();

    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    // WHAT TO WATCH FOR
    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    println!("{}", "‚ö†Ô∏è  WATCH FOR THESE ISSUES".yellow().bold());
    println!("{}", "‚îÄ".repeat(80).dimmed());
    println!();

    println!("  {} Pod fails to start", "‚ñ°".dimmed());
    println!("    ‚Üí Check logs for startup errors");
    println!("    ‚Üí Verify database connection (check secrets/configmap)");
    println!("    ‚Üí Check if migrations broke the schema");
    println!();

    println!("  {} Pod is CrashLoopBackOff", "‚ñ°".dimmed());
    println!("    ‚Üí Service is crashing on startup");
    println!("    ‚Üí Check logs for panic/error messages");
    println!("    ‚Üí Verify all environment variables are set");
    println!();

    println!("  {} Hive Router fails after update", "‚ñ°".dimmed());
    println!("    ‚Üí Check router logs for schema composition errors");
    println!("    ‚Üí Verify supergraph.graphql is valid");
    println!("    ‚Üí Ensure all subgraph URLs are correct");
    println!();

    println!("  {} GraphQL queries fail", "‚ñ°".dimmed());
    println!("    ‚Üí Hive Router may not have reloaded schema");
    println!("    ‚Üí Wait 1-2 minutes for ConfigMap update and pod restart");
    println!("    ‚Üí Try restarting hive-router pod manually if needed");
    println!();

    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    // SUMMARY
    // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    println!("{}", "‚îÅ".repeat(80).bright_blue());
    println!("{}", "üìã SUMMARY".bright_blue().bold());
    println!("{}", "‚îÅ".repeat(80).bright_blue());
    println!();

    println!(
        "  {} All build and GitOps operations completed successfully",
        "‚úì".green()
    );
    println!(
        "  {} Flux will deploy the new pod in 30-60 seconds",
        "‚è≥".yellow()
    );
    println!("  {} Hive Router will update in 1-2 minutes", "‚è≥".yellow());
    println!(
        "  {} Monitor the rollout using the commands above",
        "‚Üí".cyan()
    );
    println!();

    println!("{}", "üéâ Deployment workflow complete!".green().bold());
    println!("{}", "‚îÅ".repeat(80).bright_blue());
    println!();

    Ok(())
}

/// Full release workflow for Rust service
pub async fn release_rust_service(
    service: String,
    registry: String,
    namespace: String,
    cache_url: String,
    cache_name: String,
    _attic_token: String,
    _github_token: String,
) -> Result<()> {
    println!(
        "üöÄ {} {} {}",
        service.cyan().bold(),
        "Service Release Workflow".bold(),
        "(crate2nix)".dimmed()
    );
    println!("{}", "=".repeat(50));
    println!();

    // Load deployment configuration first (hierarchical: global ‚Üí product ‚Üí service)
    let deploy_config = DeployConfig::load_for_service(&service)?;

    // Step 0: Pre-release FluxCD health check (can be skipped via config)
    let skip_flux_health_check = deploy_config
        .service
        .deployment
        .as_ref()
        .map(|d| d.skip_flux_health_check)
        .unwrap_or(deploy_config.global.deployment.skip_flux_health_check);

    if skip_flux_health_check {
        println!(
            "Step 0/8: {}",
            "Skipping FluxCD health check (skip_flux_health_check: true)"
                .bold()
                .dimmed()
        );
    } else {
        println!("Step 0/8: {}", "Pre-release FluxCD health check...".bold());
        crate::commands::flux::health_check("pre-release").await?;
    }
    println!();

    // Compute manifest path from configuration
    let manifest_path = deploy_config.k8s_manifest_path()?;
    let manifest = manifest_path.to_string_lossy().to_string();

    println!("üìã Configuration loaded:");
    println!("   Product: {}", deploy_config.product.name);
    println!("   Environment: {}", deploy_config.product.environment);
    println!("   Registry: {}", deploy_config.registry_url());
    println!("   Namespace: {}", deploy_config.kubernetes_namespace());
    println!();

    // Tokens are set by Nix wrapper via environment variables

    // Create service configuration from deploy.yaml
    let config = ServiceConfig::from_config(service.clone(), &deploy_config);

    // Capture git tag ONCE at the start to ensure consistency across build/push/deploy
    // IMPORTANT: This prevents tag mismatch when HEAD moves between steps
    let tag_suffix = get_tag_suffix().await?;
    println!("üè∑Ô∏è  Tag suffix: {}", tag_suffix);
    println!();

    // Step 1: Build
    println!("Step 1/8: {}", "Building with per-crate caching...".bold());
    build_rust_service(
        service.clone(),
        cache_url.clone(),
        cache_name.clone(),
        String::new(),
        &deploy_config,
    )
    .await?;

    // Step 2: Push
    println!();
    println!("Step 2/9: {}", "Pushing to registries...".bold());
    push_rust_service_with_tag(
        service.clone(),
        registry.clone(),
        cache_name.clone(),
        String::new(),
        String::new(),
        tag_suffix.clone(),
    )
    .await?;

    // Step 2.5: Verify image exists in registry and capture digest
    println!();
    println!("Step 2.5/9: {}", "Verifying image in registry...".bold());
    let pushed_digest = verify_image_in_registry(&registry, &tag_suffix).await?;
    println!("   üìã Captured digest: {}", pushed_digest);

    // Step 3: Run migrations BEFORE deploying (CRITICAL: database must be ready before new pods start)
    // Check and reset stuck Shinka migrations first
    if let Ok(was_reset) = crate::commands::migrations::check_and_reset_shinka_migration(
        &deploy_config.product.name,
        &service,
        &deploy_config.kubernetes_namespace(),
    )
    .await
    {
        if was_reset {
            println!("   ‚úÖ Shinka migration reset, will retry with new image");
        }
    }

    println!();
    println!("Step 3/9: {}", "Running database migrations...".bold());
    // Format full image tag with architecture prefix
    let image_tag = format!("amd64-{}", tag_suffix);
    crate::commands::migrations::run_migrations(
        &config,
        namespace.clone(),
        image_tag,
        &deploy_config,
    )
    .await?;

    // Step 3.5: Verify image digest before deploy (race condition protection)
    println!();
    println!(
        "Step 3.5/9: {}",
        "Verifying image integrity before deploy...".bold()
    );
    verify_image_digest_matches(&registry, &tag_suffix, &pushed_digest).await?;

    // Step 4: Deploy (commits manifest changes to git) - AFTER migrations pass
    println!();
    println!("Step 4/9: {}", "Deploying via GitOps...".bold());
    let git_sha = deploy_rust_service_with_tag(
        service.clone(),
        manifest,
        registry.clone(),
        namespace.clone(),
        true,
        tag_suffix.clone(),
        None,
        None,
    )
    .await?;

    // Step 5: Flux reconcile (source + kustomization)
    println!();
    println!("Step 5/9: {}", "Syncing deployment with Flux...".bold());
    crate::commands::flux::reconcile(namespace.clone()).await?;

    // Step 5.5: Verify deployment has correct image tag after flux reconcile
    {
        let dep_name = deploy_config
            .service
            .kubernetes
            .as_ref()
            .and_then(|k| k.deployment_name.clone())
            .unwrap_or_else(|| service.clone());
        crate::commands::flux::verify_deployment_image(&namespace, &dep_name, &tag_suffix, 120)
            .await?;
    }

    // Step 6: Wait for deployment to be ready
    // Check service-level config first, then global
    let wait_for_rollout = deploy_config
        .service
        .deployment
        .as_ref()
        .map(|d| d.wait_for_rollout)
        .unwrap_or(deploy_config.global.deployment.wait_for_rollout);

    if wait_for_rollout {
        println!();
        println!("Step 6/9: {}", "Waiting for deployment rollout...".bold());
        crate::commands::flux::wait_for_deployment(
            service.clone(),
            namespace.clone(),
            deploy_config.global.deployment.deployment_wait_timeout_secs,
            tag_suffix.clone(),
            &deploy_config,
        )
        .await?;
    } else {
        println!();
        println!(
            "Step 6/9: {}",
            "Skipping deployment rollout wait (wait_for_rollout: false)".bold()
        );
    }

    // Step 6.5 (conditional): Search service GitOps sync
    // Only runs for services with novasearch.enabled = true in deploy.yaml
    if crate::commands::search_sync::should_run_novasearch_sync(&deploy_config) {
        println!();
        println!("Step 6.5/9: {}", "Running search service sync...".bold());
        let service_dir = std::path::Path::new(".");
        crate::commands::search_sync::run_novasearch_sync(service_dir, &namespace, &deploy_config)
            .await?;
    }

    // Step 7: Extract GraphQL schema
    println!();
    println!("Step 7/9: {}", "Extracting GraphQL schema...".bold());
    crate::commands::federation::extract_schema(service.clone(), &deploy_config).await?;

    // Step 8: Update GraphQL Federation (Hive Router)
    println!();
    println!("Step 8/9: {}", "Updating Hive Router federation...".bold());
    crate::commands::federation::update_federation(
        service.clone(),
        namespace.clone(),
        &deploy_config,
    )
    .await?;

    // Step 7.4: Auto-release federation-tests if this service has federation tests enabled
    // This ensures the test image is current before running tests
    let federation_tests_tag_override = if deploy_config.service.federation_tests.enabled
        && !deploy_config.service.federation_tests_service.is_global
    {
        println!();
        println!(
            "Step 8.4/9: {}",
            "Releasing federation-tests image...".bold()
        );

        // Capture the current git SHA BEFORE starting the federation-tests release
        // We'll need to detect the architecture after the build completes
        let git_sha_before_release = crate::commands::rust_service::get_tag_suffix().await?;

        // Find the monorepo root
        let repo_root = crate::git::get_repo_root()?;

        // Construct the service directory path from config
        // We know: repo_root, product name, service name
        // Structure: {repo_root}/pkgs/products/{product}/services/rust/{service}
        let service_dir = repo_root
            .join("pkgs")
            .join("products")
            .join(&deploy_config.product.name)
            .join("services")
            .join("rust")
            .join(&service);

        // Construct path to federation-tests: pkgs/products/{product}/tests/federation
        let federation_tests_dir = repo_root
            .join("pkgs")
            .join("products")
            .join(&deploy_config.product.name)
            .join("tests")
            .join("federation");

        if !federation_tests_dir.exists() {
            bail!(
                "Federation tests directory not found: {}\n  \
                 Expected structure: pkgs/products/{}/tests/federation",
                federation_tests_dir.display(),
                deploy_config.product.name
            );
        }

        println!("   üìÅ Federation tests: {}", federation_tests_dir.display());

        // Run nix run .#release from the federation-tests directory
        let status = std::process::Command::new("nix")
            .args(&["run", ".#release"])
            .current_dir(&federation_tests_dir)
            .status()
            .context("Failed to execute nix run .#release for federation-tests")?;

        if !status.success() {
            bail!(
                "Federation-tests release failed with exit code: {:?}",
                status.code()
            );
        }

        println!("   ‚úÖ Federation-tests image released");

        // Detect which architecture was built by checking result files
        // The build creates result-amd64 or result-arm64 symlinks
        let full_image_tag = {
            let amd64_result = federation_tests_dir.join("result-amd64");
            let arm64_result = federation_tests_dir.join("result-arm64");

            if amd64_result.exists() && arm64_result.exists() {
                // Both architectures built - use multi-arch tag (just the SHA)
                // This should match whatever the push step uses for multi-arch manifests
                git_sha_before_release.clone()
            } else if amd64_result.exists() {
                format!("amd64-{}", git_sha_before_release)
            } else if arm64_result.exists() {
                format!("arm64-{}", git_sha_before_release)
            } else {
                // Fallback to just SHA if no result files found
                git_sha_before_release.clone()
            }
        };

        println!("   üìã Detected image tag: {}", full_image_tag);

        // Now update THIS SERVICE's deploy.yaml with the federation-tests image tag

        println!();
        println!(
            "   üìù Updating {}'s deploy.yaml with federation-tests tag...",
            service
        );
        let fed_product_dir = repo_root
            .join("pkgs")
            .join("products")
            .join(&deploy_config.product.name);
        let fed_deploy_yaml = resolve_deploy_yaml_path(&fed_product_dir, &service, &service_dir);

        update_service_federation_tests_tag(
            &service,
            &full_image_tag,
            &deploy_config,
            &fed_deploy_yaml,
        )
        .await?;

        // Commit the updated service deploy.yaml
        let service_deploy_yaml = fed_deploy_yaml;
        crate::git::commit_and_push(
            &service_deploy_yaml,
            "", // old_tag not needed
            &full_image_tag,
        )?;
        println!("   ‚úÖ Service deploy.yaml updated and committed");

        // Return the full tag so it can be passed to Step 8.5
        // This includes the architecture prefix (e.g., "amd64-347a310176")
        Some(full_image_tag)
    } else {
        None
    };

    // Step 8.5: Run federation integration tests (AFTER deployment is complete)
    if deploy_config.service.federation_tests.enabled {
        println!();
        println!(
            "Step 8.5/9: {}",
            "Running federation integration tests...".bold()
        );

        crate::commands::federation_tests::run_federation_tests(
            &service,
            &deploy_config.product.name,
            &deploy_config.product.environment,
            &namespace,
            &deploy_config.service.federation_tests.suite,
            &deploy_config.service.federation_tests.router_url,
            deploy_config.service.federation_tests.timeout_seconds,
            deploy_config.service.federation_tests.fail_fast,
            &git_sha,
            &deploy_config,
            federation_tests_tag_override.as_deref(),
        )
        .await?;
    } else {
        println!();
        println!(
            "Step 8.5/9: {} {}",
            "Skipping federation integration tests".dimmed(),
            "(not enabled in deploy.yaml)".dimmed()
        );
    }

    // Generate comprehensive deployment report
    println!();
    println!("{}", "‚îÅ".repeat(80).bright_blue());
    println!("üìä {}", "DEPLOYMENT REPORT".bright_blue().bold());
    println!("{}", "‚îÅ".repeat(80).bright_blue());
    print_deployment_report(&service, &namespace, &deploy_config, &tag_suffix).await?;

    // Step 9: Post-release FluxCD health check with retry (can be skipped via config)
    // CRITICAL: Verify GitOps system is still healthy after deployment
    // Wait for Flux to finish reconciling changes before declaring success
    // This ensures the release didn't break the cluster reconciliation
    println!();
    if skip_flux_health_check {
        println!(
            "Step 9/9: {}",
            "Skipping post-release FluxCD health check (skip_flux_health_check: true)"
                .bold()
                .dimmed()
        );
    } else {
        println!("Step 9/9: {}", "Post-release FluxCD health check...".bold());
        // Wait up to 10 minutes for all kustomizations to reconcile (was 5 minutes)
        // Longer timeout handles complex deployments with multiple services
        crate::commands::flux::health_check_with_retry("post-release", 600, 10).await?;
    }

    println!();
    println!("{}", "‚îÅ".repeat(80).bright_green());
    println!("{}", "‚úÖ RELEASE COMPLETE".green().bold());
    println!("{}", "‚îÅ".repeat(80).bright_green());

    Ok(())
}

/// Update service's deploy.yaml with new federation-tests image tag
///
/// This function is called after Step 8.4 releases federation-tests.
/// It updates the service's own deploy.yaml to use the freshly built federation-tests image.
async fn update_service_federation_tests_tag(
    _service: &str,
    tag_suffix: &str,
    _deploy_config: &DeployConfig,
    deploy_yaml_path: &std::path::Path,
) -> Result<()> {
    if !deploy_yaml_path.exists() {
        bail!("deploy.yaml not found at: {}", deploy_yaml_path.display());
    }

    println!("   üìù Updating deploy.yaml...");

    // Read current content
    let content =
        std::fs::read_to_string(&deploy_yaml_path).context("Failed to read deploy.yaml")?;

    // Update or add the image_tag line under federation_tests section
    let mut updated_content = String::new();
    let mut updated = false;
    let mut in_federation_tests_section = false;
    let mut found_image_tag = false;

    for line in content.lines() {
        if line.trim_start().starts_with("federation_tests:") {
            in_federation_tests_section = true;
            updated_content.push_str(line);
            updated_content.push('\n');
        } else if in_federation_tests_section {
            // Check if we're still in the federation_tests section (indented)
            if !line.starts_with(' ') && !line.trim().is_empty() {
                // We've exited the section - add image_tag if not found
                if !found_image_tag {
                    updated_content.push_str(&format!("  image_tag: \"{}\"\n", tag_suffix));
                    updated = true;
                }
                in_federation_tests_section = false;
                updated_content.push_str(line);
                updated_content.push('\n');
            } else if line.trim_start().starts_with("image_tag:") {
                // Replace the existing image_tag line
                let indent = line.len() - line.trim_start().len();
                updated_content.push_str(&" ".repeat(indent));
                updated_content.push_str(&format!("image_tag: \"{}\"\n", tag_suffix));
                updated = true;
                found_image_tag = true;
            } else {
                updated_content.push_str(line);
                updated_content.push('\n');
            }
        } else {
            updated_content.push_str(line);
            updated_content.push('\n');
        }
    }

    // If we reached EOF while still in federation_tests section and didn't find image_tag
    if in_federation_tests_section && !found_image_tag {
        updated_content.push_str(&format!("  image_tag: \"{}\"\n", tag_suffix));
        updated = true;
    }

    if !updated {
        bail!("Could not update image_tag in federation_tests section of deploy.yaml");
    }

    // Write back
    std::fs::write(&deploy_yaml_path, updated_content).context("Failed to write deploy.yaml")?;

    println!(
        "   ‚úÖ Updated federation_tests.image_tag to: {}",
        tag_suffix
    );

    Ok(())
}
